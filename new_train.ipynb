{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1128d0cc",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning for Qwen3-VL on Dots-and-Boxes\n",
    "This notebook fine-tunes `Qwen/Qwen3-VL-4B-Instruct` using PEFT-LoRA. It includes a custom multimodal collator so the HF Trainer handles both image and text correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data' / 'prepared'\n",
    "TRAIN_FILE = DATA_DIR / 'dataset_train.json'\n",
    "EVAL_FILE  = DATA_DIR / 'dataset_eval.json'\n",
    "CHECKPOINTS_DIR = BASE_DIR / 'checkpoints'\n",
    "\n",
    "MODEL_ID = 'Qwen/Qwen3-VL-4B-Instruct'\n",
    "\n",
    "seq = 0\n",
    "while (CHECKPOINTS_DIR / f'lora_{seq}').exists():\n",
    "    seq += 1\n",
    "OUTPUT_DIR = CHECKPOINTS_DIR / f'lora_{seq}'\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "print('Device:', device, 'BF16:', bf16, 'OUTPUT_DIR:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e205ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and processor\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16 if bf16 else (torch.float16 if device=='cuda' else torch.float32),\n",
    "    device_map='auto',\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "print('Model and processor loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and multimodal collator (simple functions)\n",
    "with open(TRAIN_FILE, 'r', encoding='utf-8') as f:\n",
    "    train_dataset = json.load(f)\n",
    "with open(EVAL_FILE, 'r', encoding='utf-8') as f:\n",
    "    eval_dataset = json.load(f)\n",
    "    \n",
    "print('Train/Eval sizes:', len(train_dataset), len(eval_dataset))\n",
    "\n",
    "def multimodal_collator(batch: List[Dict]):\n",
    "    images = []\n",
    "    full_texts = []\n",
    "    prompt_texts = []\n",
    "    for sample in batch:\n",
    "        msgs = sample['messages']\n",
    "        # Load image path into PIL for the processor\n",
    "        pil_img = None\n",
    "        \n",
    "        for content_item in msgs[0]['content']:\n",
    "            if content_item.get('type') == 'image':\n",
    "                img_path = Path(content_item.get('image'))\n",
    "                if not img_path.is_absolute():\n",
    "                    img_path = (BASE_DIR / img_path).resolve()\n",
    "                pil_img = Image.open(img_path).convert('RGB')\n",
    "                content_item['image'] = pil_img\n",
    "                break\n",
    "        images.append(pil_img)\n",
    "        full_text = processor.apply_chat_template(\n",
    "            msgs, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        prompt_text = processor.apply_chat_template(\n",
    "            msgs[:-1], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        full_texts.append(full_text)\n",
    "        prompt_texts.append(prompt_text)\n",
    "\n",
    "    full_out = processor(\n",
    "        text=full_texts, images=images, return_tensors='pt', padding=True\n",
    "    )\n",
    "    prompt_out = processor(\n",
    "        text=prompt_texts, images=images, return_tensors='pt', padding=True\n",
    "    )\n",
    "    labels = full_out['input_ids'].clone()\n",
    "    prompt_lens = prompt_out['attention_mask'].sum(dim=1).tolist()\n",
    "    for i, L in enumerate(prompt_lens):\n",
    "        labels[i, :int(L)] = -100\n",
    "    \n",
    "    batch_out = dict(full_out)\n",
    "    batch_out['labels'] = labels\n",
    "    \n",
    "    \n",
    "    return batch_out\n",
    "\n",
    "\n",
    "collator = multimodal_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and attach LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj']\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    eval_steps=4,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='epoch',\n",
    "    bf16=bf16,\n",
    "    fp16=(not bf16 and device=='cuda'),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=['none'],\n",
    "    gradient_checkpointing=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "print('Trainer ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save adapters\n",
    "train_result = trainer.train()\n",
    "trainer.save_model(str(OUTPUT_DIR))\n",
    "processor.save_pretrained(str(OUTPUT_DIR))\n",
    "with open(OUTPUT_DIR / 'train_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'metrics': train_result.metrics,\n",
    "        'output_dir': str(OUTPUT_DIR)\n",
    "    }, f, indent=2)\n",
    "print('Saved to', OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
